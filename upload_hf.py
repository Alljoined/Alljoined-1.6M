from dotenv import load_dotenv
from huggingface_hub import HfApi, CommitOperationDelete
import os
import shutil
from huggingface_hub.hf_api import RepoFile   # <-- type guard
import fnmatch

load_dotenv()
api = HfApi(token=os.getenv("HF_TOKEN"))

root = "/srv/eeg_reconstruction/shared/data/raw_eeg/Alljoined-1.6M"
for sub in sorted(os.listdir(root)):              # e.g. sub-17, sub-18, …
    path = os.path.join(root, sub)
    if not os.path.isdir(path):
        continue
    api.upload_folder(
        folder_path=path,
        path_in_repo= f"raw_eeg/{sub}",                          # lands in the same-named dir online
        repo_id="Alljoined/AJ-1.6M",
        repo_type="dataset",
        commit_message=f"add {sub} raw data",
    )

root = "/srv/eeg_reconstruction/shared/data/preprocessed_data/Alljoined-1.6M"
for sub in sorted(os.listdir(root)):              # e.g. sub-17, sub-18, …
    path = os.path.join(root, sub)
    if not os.path.isdir(path):
        continue
    if not sub.startswith("sub-"):
        continue
    api.upload_folder(
        folder_path=path,
        path_in_repo= f"preprocessed_eeg/{sub}",                         # lands in the same-named dir online
        repo_id="Alljoined/AJ-1.6M", 
        repo_type="dataset",
        commit_message=f"add {sub} preprocessed data",
    )

api.upload_large_folder(
    folder_path="/srv/eeg_reconstruction/shared/data/stimuli/repr_eeg2/images",
    path_in_repo="stimlui",
    repo_id="Alljoined/AJ-1.6M",
    repo_type="dataset",
    commit_message="add stimlui",
)


api.upload_file(
    path_or_fileobj="/srv/eeg_reconstruction/shared/data/stimuli/repr_eeg2/images.zip",
    path_in_repo="stimuli.zip",
    repo_id="Alljoined/AJ-1.6M",
    repo_type="dataset",
)

# # glob patterns to remove
# PATTERNS = [
#     "**/mvnn_whitening_matrices.pkl",
#     "**/experiment_metadata.parquet",
#     "**/stim_order.parquet",
#     "**/metadata.parquet",
# ]

# # 1) walk the repo tree once
# tree = api.list_repo_tree(
#     repo_id="Alljoined/AJ-1.6M",
#     repo_type="dataset",
#     recursive=True,           # deep list
# )

# # 2) build delete operations for the matches
# ops = [
#     CommitOperationDelete(path_in_repo=f.path)
#     for f in tree
#     if isinstance(f, RepoFile)                               # skip folders
#     and any(fnmatch.fnmatch(f.path, pat) for pat in PATTERNS)
# ]

# print(f"Matched {len(ops)} files for deletion")

# if ops:                                                 # avoid empty-commit error
#     api.create_commit(
#         repo_id="Alljoined/AJ-1.6M",
#         repo_type="dataset",
#         operations=ops,
#         commit_message="Remove autogenerated metadata (.parquet, .pkl)",
#     )